{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelo Bertimbau para avaliação das proposições"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, os, evaluate, wandb,datetime\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Dataset, load_dataset\n",
    "import pandas as pd\n",
    "from google.colab import drive\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, roc_auc_score, \\\n",
    "                            classification_report, f1_score, precision_recall_fscore_support\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification,\\\n",
    "                            Trainer,TrainingArguments, AutoConfig, EarlyStoppingCallback, IntervalStrategy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch import cuda\n",
    "device = 'cuda' if cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carregar e organizar o dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#caminho para o drive com os dados de treinamento\n",
    "BASE_PATH = \"/content/drive/MyDrive/IA-AzMina/modelos/\"\n",
    "RANDOM_SEED = 5151\n",
    "\n",
    "dataset = load_dataset(\"azmina/ementas_congresso\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#organizando o dataset com as colunas que serão utilizadas e dividindo entre treino e teste\n",
    "cols = [\"text\",\"label_desfavoravel\"]\n",
    "fix_columns = {\"label_desfavoravel\":\"label\"}\n",
    "\n",
    "df_test = df[\"test\"].to_pandas().dropna(subset=['textoInteiroTeor'])[cols].rename(columns=fix_columns)\n",
    "df_train = df[\"train\"].to_pandas()[cols].rename(columns=fix_columns)\n",
    "df_val = df[\"val\"].to_pandas()[cols].rename(columns=fix_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#carregando o tokenizer e o modelo\n",
    "unique_labels = df_train.label.unique()\n",
    "\n",
    "label2id = {str(label): int(i) for i, label in enumerate(unique_labels)}\n",
    "id2label = {int(i): str(label) for i, label in enumerate(unique_labels)}\n",
    "\n",
    "pretrainedmodel = 'neuralmind/bert-base-portuguese-cased'\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(pretrainedmodel,\n",
    "                                                           num_labels=len(unique_labels),\n",
    "                                                           id2label=id2label,\n",
    "                                                           label2id=label2id,\n",
    "                                                           hidden_dropout_prob=0.07,\n",
    "                                                           attention_probs_dropout_prob=0.07)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrainedmodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(record,max_length=512):\n",
    "    return tokenizer(record['text'], truncation=True, padding='max_length', max_length=max_length)\n",
    "\n",
    "train = Dataset.from_pandas(df_train).map(lambda x: tokenize_function(x, max_length=512), batched=True)\n",
    "val = Dataset.from_pandas(df_val).map(lambda x: tokenize_function(x, max_length=512), batched=True)\n",
    "test = Dataset.from_pandas(df_test).map(lambda x: tokenize_function(x, max_length=512), batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definindo as métricas de avaliação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_metric = evaluate.load(\"accuracy\")\n",
    "f1_metric = evaluate.load(\"f1\")\n",
    "recall_metric = evaluate.load(\"recall\")\n",
    "precision_metric = evaluate.load(\"precision\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "\n",
    "    accuracy_result = accuracy_metric.compute(predictions=predictions, references=labels)\n",
    "    f1_result = f1_metric.compute(predictions=predictions, references=labels,average='macro')\n",
    "    recall_result = recall_metric.compute(predictions=predictions, references=labels,average='macro')\n",
    "    precision_result = precision_metric.compute(predictions=predictions, references=labels,average='macro')\n",
    "\n",
    "    result = {**accuracy_result, **f1_result, **recall_result, **precision_result}\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Treinamento do modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "eval_steps = len(train) // batch_size * 3 # aumente o multiplicador para avaliar menos\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\".\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    eval_steps=eval_steps,\n",
    "    save_strategy=\"epoch\",\n",
    "    save_steps=eval_steps*2,\n",
    "    warmup_steps=150,\n",
    "    logging_steps=10,\n",
    "    learning_rate=1e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=19,\n",
    "    weight_decay=0.02,\n",
    "    seed=RANDOM_SEED,\n",
    "    save_total_limit=1,\n",
    "    report_to=\"wandb\",\n",
    "    metric_for_best_model=\"f1\",\n",
    "    run_name=wandb_run_name,\n",
    "    load_best_model_at_end = True,\n",
    "    optim=\"adamw_torch\"\n",
    ")\n",
    "\n",
    "trainer = CustomTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train,\n",
    "    eval_dataset=val,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Avaliação no conjunto de teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.evaluate(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_test = test.to_pandas()\n",
    "eval_test[\"pred_label\"] = eval_test[\"text\"].apply(lambda x: predict_sentence(x))\n",
    "eval_test[\"pred_label\"].value_counts()\n",
    "\n",
    "eval_test[\"label_name\"] = eval_test[\"label\"].apply(lambda x: id2label[x])\n",
    "\n",
    "print(classification_report(eval_test[\"label_name\"], eval_test[\"pred_label\"]))\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
